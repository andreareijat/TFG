{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISIS DE REDES NEURONALES PREENTRENADAS PARA LA ESTIMACIÓN DE PROFUNDIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pretende realizar el análisis de una serie de redes neuronales preentrenadas para la estimación de profundidad que son: \n",
    "- MonoDepth2\n",
    "- MiDAS\n",
    "- SC-DepthV2\n",
    "- DenseDepth\n",
    "\n",
    "Para poder comprobar su precisión se han sacado unas imágenes de test con una cámara Interl RealSense tanto en modo monocular como en RGBD que se han guardado en sus respectivas carpetas ./monocular_photos y ./rgbd_photos. Las imágenes monoculares servirán como entrada para las redes y las rgbd se emplearán como ground truth y así obtener la precisión de los mapas de profundidad de la salida de las redes.\n",
    "\n",
    "A mayores también se pueden hacer pruebas con los datasets de repositorios públicos como: \n",
    "\n",
    "- https://theairlab.org/tartanair-dataset/\n",
    "- https://cvg.cit.tum.de/data/datasets/rgbd-dataset/download\n",
    "- https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\n",
    "- https://www.cvlibs.net/datasets/kitti/\n",
    "\n",
    "Sin embargo, el problema es que en este caso puede que alguna de las redes haya sido entrenada con las imágenes de alguno o varios de esos repositorios, y por lo tanto el test no será fiable. Deberíamos de estar seguros que las imágenes nunca hayan sido vistas por las redes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MonoDepth2\n",
    "\n",
    "https://github.com/nianticlabs/monodepth2\n",
    "\n",
    "En concreto el modelo mono_640x192 está entrenado solo con imágenes monoculares, sin información estéreo. <br>\n",
    "Por otro lado, el modelo mono+stereo_640x192 está entrenado usando tanto imágenes monoculares como pares estéreo durante el entrenamiento. <br>\n",
    "Aunque solo utiliza monoculares para la predicción, el entrenamiento con datos estéreo puede ayudar a mejorar la precisión. <br><br>\n",
    "\n",
    "Por lo tanto, el mejor modelo y el escogido para mi caso es el mono+stereo_640x192."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descarga del modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Downloading pretrained model to models/mono+stereo_640x192.zip\n",
      "   Unzipping model...\n",
      "   Model unzipped to models/mono+stereo_640x192\n",
      "-> Loading model from  models/mono+stereo_640x192\n",
      "   Loading pretrained encoder\n",
      "/home/andrea/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/andrea/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "   Loading pretrained decoder\n",
      "Traceback (most recent call last):\n",
      "  File \"./test_simple.py\", line 171, in <module>\n",
      "    test_simple(args)\n",
      "  File \"./test_simple.py\", line 112, in test_simple\n",
      "    raise Exception(\"Can not find args.image_path: {}\".format(args.image_path))\n",
      "Exception: Can not find args.image_path: monocular_photos\n"
     ]
    }
   ],
   "source": [
    "!python3 ./test_simple.py --image_path monocular_photos --model_name mono+stereo_640x192 --ext jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la carpeta de imágenes y resultados\n",
    "input_folder = './monocular_photos'\n",
    "output_folder = './network_depth_maps'\n",
    "model_name = 'mono+stereo_640x192'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model_if_doesnt_exist(model_name):\n",
    "    \"\"\"If pretrained kitti model doesn't exist, download and unzip it\n",
    "    \"\"\"\n",
    "    # values are tuples of (<google cloud URL>, <md5 checksum>)\n",
    "    download_paths = {\n",
    "        \"mono_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip\",\n",
    "             \"a964b8356e08a02d009609d9e3928f7c\"),\n",
    "        \"stereo_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip\",\n",
    "             \"3dfb76bcff0786e4ec07ac00f658dd07\"),\n",
    "        \"mono+stereo_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip\",\n",
    "             \"c024d69012485ed05d7eaa9617a96b81\"),\n",
    "        \"mono_no_pt_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip\",\n",
    "             \"9c2f071e35027c895a4728358ffc913a\"),\n",
    "        \"stereo_no_pt_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip\",\n",
    "             \"41ec2de112905f85541ac33a854742d1\"),\n",
    "        \"mono+stereo_no_pt_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip\",\n",
    "             \"46c3b824f541d143a45c37df65fbab0a\"),\n",
    "        \"mono_1024x320\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip\",\n",
    "             \"0ab0766efdfeea89a0d9ea8ba90e1e63\"),\n",
    "        \"stereo_1024x320\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip\",\n",
    "             \"afc2f2126d70cf3fdf26b550898b501a\"),\n",
    "        \"mono+stereo_1024x320\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip\",\n",
    "             \"cdc5fc9b23513c07d5b19235d9ef08f7\"),\n",
    "        }\n",
    "\n",
    "    if not os.path.exists(\"./monodepth2/models\"):\n",
    "        os.makedirs(\"models\")\n",
    "\n",
    "    model_path = os.path.join(\"./monodepth2/models\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonodepth2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m networks\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonodepth2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Cargar el modelo\u001b[39;00m\n",
      "File \u001b[0;32m~/Escritorio/TFG_ANDREA/TFG/monodepth2/networks/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResnetEncoder\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdepth_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepthDecoder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpose_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PoseDecoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpose_cnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PoseCNN\n",
      "File \u001b[0;32m~/Escritorio/TFG_ANDREA/TFG/monodepth2/networks/depth_decoder.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDepthDecoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_ch_enc, scales\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m), num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, use_skips\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layers'"
     ]
    }
   ],
   "source": [
    "from monodepth2 import networks\n",
    "from monodepth2 import layers\n",
    "\n",
    "# Cargar el modelo\n",
    "download_model_if_doesnt_exist(model_name)\n",
    "encoder_path = os.path.join(\"./monodepth2/models\", model_name, \"encoder.pth\")\n",
    "depth_decoder_path = os.path.join(\"./monodepth2/models\", model_name, \"depth.pth\")\n",
    "\n",
    "# # Cargando componentes del modelo\n",
    "# encoder = MonoDecoder(num_input_images=1)\n",
    "# depth_decoder = DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "# loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "# filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "# encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "# loaded_dict = torch.load(depth_decoder_path, map_location='cpu')\n",
    "# depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "# encoder.eval()\n",
    "# depth_decoder.eval()\n",
    "\n",
    "# # Cargar y procesar cada imagen\n",
    "# for image_file in os.listdir(input_folder):\n",
    "#     if image_file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "#         image_path = os.path.join(input_folder, image_file)\n",
    "#         input_image = cv2.imread(image_path)\n",
    "#         input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "#         input_image = cv2.resize(input_image, (640, 192))\n",
    "#         input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             features = encoder(input_image)\n",
    "#             outputs = depth_decoder(features)\n",
    "\n",
    "#         disp = outputs[(\"disp\", 0)]\n",
    "#         disp_resized = torch.nn.functional.interpolate(\n",
    "#             disp, (input_image.shape[2], input_image.shape[3]), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "#         # Normalizar el mapa de profundidad para visualización\n",
    "#         disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "#         vmax = np.percentile(disp_resized_np, 95)\n",
    "#         normal_depth = np.clip(disp_resized_np / vmax, 0, 1)\n",
    "#         depth_colormap = (normal_depth * 255).astype(np.uint8)\n",
    "#         depth_colormap = cv2.applyColorMap(depth_colormap, cv2.COLORMAP_MAGMA)\n",
    "\n",
    "#         output_path = os.path.join(output_folder, image_file)\n",
    "#         cv2.imwrite(output_path, depth_colormap)\n",
    "\n",
    "# print(\"Procesamiento completado. Mapas de profundidad guardados en:\", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Predicting on 41 test images\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "if os.path.isdir(input_folder):\n",
    "    # Searching folder for images\n",
    "    paths = glob.glob(os.path.join(input_folder, '*.{}'.format('jpg')))\n",
    "    output_directory = input_folder\n",
    "else:\n",
    "    raise Exception(\"Can not find args.image_path: {}\".format(input_folder))\n",
    "\n",
    "print(\"-> Predicting on {:d} test images\".format(len(paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonodepth2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disp_to_depth\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonodepth2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEREO_SCALE_FACTOR\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# PREDICTING ON EACH IMAGE IN TURN\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Escritorio/TFG/monodepth2/evaluate_depth.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disp_to_depth\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m readlines\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MonodepthOptions\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layers'"
     ]
    }
   ],
   "source": [
    "from monodepth2.layers import disp_to_depth\n",
    "from monodepth2.evaluate_depth import STEREO_SCALE_FACTOR\n",
    "# PREDICTING ON EACH IMAGE IN TURN\n",
    "with torch.no_grad():\n",
    "    for idx, image_path in enumerate(paths):\n",
    "\n",
    "        if image_path.endswith(\"_disp.jpg\"):\n",
    "            # don't try to predict disparity for a disparity image!\n",
    "            continue\n",
    "\n",
    "        # Load image and preprocess\n",
    "        input_image = Image.open(image_path).convert('RGB')\n",
    "        original_width, original_height = input_image.size\n",
    "        input_image = input_image.resize((feed_width, feed_height), Image.LANCZOS)\n",
    "        input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "        # PREDICTION\n",
    "        input_image = input_image.to(device)\n",
    "        features = encoder(input_image)\n",
    "        outputs = depth_decoder(features)\n",
    "\n",
    "        disp = outputs[(\"disp\", 0)]\n",
    "        disp_resized = torch.nn.functional.interpolate(\n",
    "            disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Saving numpy file\n",
    "        output_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        scaled_disp, depth = disp_to_depth(disp, 0.1, 100)\n",
    "\n",
    "        name_dest_npy = os.path.join(output_folder, \"{}_depth.npy\".format(output_name))\n",
    "        metric_depth = STEREO_SCALE_FACTOR * depth.device().numpy()\n",
    "        np.save(name_dest_npy, metric_depth)\n",
    "\n",
    "        # Saving colormapped depth image\n",
    "        disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "        vmax = np.percentile(disp_resized_np, 95)\n",
    "        normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n",
    "        mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')\n",
    "        colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n",
    "        im = Image.fromarray(colormapped_im)\n",
    "\n",
    "        name_dest_im = os.path.join(output_directory, \"{}_disp.jpeg\".format(output_name))\n",
    "        im.save(name_dest_im)\n",
    "\n",
    "        print(\"   Processed {:d} of {:d} images - saved predictions to:\".format(\n",
    "            idx + 1, len(paths)))\n",
    "        print(\"   - {}\".format(name_dest_im))\n",
    "        print(\"   - {}\".format(name_dest_npy))\n",
    "\n",
    "print('-> Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import networks\n",
    "\n",
    "# Configuración de la carpeta de imágenes y resultados\n",
    "input_folder = './monocular_photos'\n",
    "output_folder = './depth_maps'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Cargar el modelo\n",
    "model_path = 'models/mono+stereo_640x192'\n",
    "encoder_path = os.path.join(model_path, 'encoder.pth')\n",
    "depth_decoder_path = os.path.join(model_path, 'depth.pth')\n",
    "\n",
    "encoder = networks.ResnetEncoder(18, False)\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "depth_decoder.load_state_dict(torch.load(depth_decoder_path, map_location='cpu'))\n",
    "\n",
    "encoder.eval()\n",
    "depth_decoder.eval()\n",
    "\n",
    "# Procesar imágenes\n",
    "with torch.no_grad():\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        if image_file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            input_image = Image.open(os.path.join(input_folder, image_file)).convert('RGB')\n",
    "            original_width, original_height = input_image.size\n",
    "            input_image = input_image.resize((640, 192), Image.LANCZOS)\n",
    "            input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "            # Predicción de profundidad\n",
    "            features = encoder(input_image)\n",
    "            outputs = depth_decoder(features)\n",
    "\n",
    "            disp = outputs[(\"disp\", 0)]\n",
    "            disp_resized = torch.nn.functional.interpolate(\n",
    "                disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "            disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "\n",
    "            # Guardar el resultado\n",
    "            depth_filename = os.path.join(output_folder, os.path.splitext(image_file)[0] + '_depth.png')\n",
    "            Image.fromarray((disp_resized_np * 255).astype(np.uint8)).save(depth_filename)\n",
    "\n",
    "print(\"Procesamiento completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
