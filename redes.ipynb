{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISIS DE REDES NEURONALES PREENTRENADAS PARA LA ESTIMACIÓN DE PROFUNDIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pretende realizar el análisis de una serie de redes neuronales preentrenadas para la estimación de profundidad que son: \n",
    "- MonoDepth2\n",
    "- MiDAS\n",
    "- SC-DepthV2\n",
    "- DenseDepth\n",
    "\n",
    "Para poder comprobar su precisión se han sacado unas imágenes de test con una cámara Interl RealSense tanto en modo monocular como en RGBD que se han guardado en sus respectivas carpetas ./monocular_photos y ./rgbd_photos. Las imágenes monoculares servirán como entrada para las redes y las rgbd se emplearán como ground truth y así obtener la precisión de los mapas de profundidad de la salida de las redes.\n",
    "\n",
    "A mayores también se pueden hacer pruebas con los datasets de repositorios públicos como: \n",
    "\n",
    "- https://theairlab.org/tartanair-dataset/\n",
    "- https://cvg.cit.tum.de/data/datasets/rgbd-dataset/download\n",
    "- https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\n",
    "- https://www.cvlibs.net/datasets/kitti/\n",
    "\n",
    "Sin embargo, el problema es que en este caso puede que alguna de las redes haya sido entrenada con las imágenes de alguno o varios de esos repositorios, y por lo tanto el test no será fiable. Deberíamos de estar seguros que las imágenes nunca hayan sido vistas por las redes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MonoDepth2\n",
    "\n",
    "https://github.com/nianticlabs/monodepth2\n",
    "\n",
    "En concreto el modelo mono_640x192 está entrenado solo con imágenes monoculares, sin información estéreo. <br>\n",
    "Por otro lado, el modelo mono+stereo_640x192 está entrenado usando tanto imágenes monoculares como pares estéreo durante el entrenamiento. <br>\n",
    "Aunque solo utiliza monoculares para la predicción, el entrenamiento con datos estéreo puede ayudar a mejorar la precisión. <br><br>\n",
    "\n",
    "Por lo tanto, el mejor modelo y el escogido para mi caso es el mono+stereo_640x192."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descarga del modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Downloading pretrained model to models/mono+stereo_640x192.zip\n",
      "   Unzipping model...\n",
      "   Model unzipped to models/mono+stereo_640x192\n",
      "-> Loading model from  models/mono+stereo_640x192\n",
      "   Loading pretrained encoder\n",
      "/home/andrea/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/andrea/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "   Loading pretrained decoder\n",
      "-> Predicting on 41 test images\n",
      "/home/andrea/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "   Processed 1 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111615_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111615_disp.npy\n",
      "   Processed 2 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111633_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111633_disp.npy\n",
      "   Processed 3 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111916_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111916_disp.npy\n",
      "   Processed 4 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111651_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111651_disp.npy\n",
      "   Processed 5 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111914_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111914_disp.npy\n",
      "   Processed 6 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111350_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111350_disp.npy\n",
      "   Processed 7 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111856_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111856_disp.npy\n",
      "   Processed 8 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111530_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111530_disp.npy\n",
      "   Processed 9 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111501_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111501_disp.npy\n",
      "   Processed 10 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111506_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111506_disp.npy\n",
      "   Processed 11 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111904_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111904_disp.npy\n",
      "   Processed 12 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111936_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111936_disp.npy\n",
      "   Processed 13 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111911_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111911_disp.npy\n",
      "   Processed 14 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111439_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111439_disp.npy\n",
      "   Processed 15 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111452_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111452_disp.npy\n",
      "   Processed 16 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111444_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111444_disp.npy\n",
      "   Processed 17 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111607_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111607_disp.npy\n",
      "   Processed 18 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111850_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111850_disp.npy\n",
      "   Processed 19 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111422_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111422_disp.npy\n",
      "   Processed 20 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111257_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111257_disp.npy\n",
      "   Processed 21 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111922_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111922_disp.npy\n",
      "   Processed 22 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111515_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111515_disp.npy\n",
      "   Processed 23 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111549_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111549_disp.npy\n",
      "   Processed 24 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111411_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111411_disp.npy\n",
      "   Processed 25 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111832_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111832_disp.npy\n",
      "   Processed 26 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111245_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111245_disp.npy\n",
      "   Processed 27 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111909_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111909_disp.npy\n",
      "   Processed 28 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111706_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111706_disp.npy\n",
      "   Processed 29 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111840_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111840_disp.npy\n",
      "   Processed 30 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111713_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111713_disp.npy\n",
      "   Processed 31 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111929_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111929_disp.npy\n",
      "   Processed 32 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111646_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111646_disp.npy\n",
      "   Processed 33 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111540_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111540_disp.npy\n",
      "   Processed 34 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111547_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111547_disp.npy\n",
      "   Processed 35 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111700_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111700_disp.npy\n",
      "   Processed 36 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111600_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111600_disp.npy\n",
      "   Processed 37 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111341_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111341_disp.npy\n",
      "   Processed 38 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111836_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111836_disp.npy\n",
      "   Processed 39 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111631_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111631_disp.npy\n",
      "   Processed 40 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111931_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111931_disp.npy\n",
      "   Processed 41 of 41 images - saved predictions to:\n",
      "   - monocular_photos/color-20240503-111849_disp.jpeg\n",
      "   - monocular_photos/color-20240503-111849_disp.npy\n",
      "-> Done!\n"
     ]
    }
   ],
   "source": [
    "!python3 ./monodepth2-master/test_simple.py --image_path monocular_photos --model_name mono+stereo_640x192 --ext jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la carpeta de imágenes y resultados\n",
    "input_folder = './monocular_photos'\n",
    "output_folder = './network_depth_maps'\n",
    "model_name = 'mono+stereo_640x192'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model_if_doesnt_exist(model_name):\n",
    "    \"\"\"If pretrained kitti model doesn't exist, download and unzip it\n",
    "    \"\"\"\n",
    "    # values are tuples of (<google cloud URL>, <md5 checksum>)\n",
    "    download_paths = {\n",
    "        \"mono_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip\",\n",
    "             \"a964b8356e08a02d009609d9e3928f7c\"),\n",
    "        \"stereo_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip\",\n",
    "             \"3dfb76bcff0786e4ec07ac00f658dd07\"),\n",
    "        \"mono+stereo_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip\",\n",
    "             \"c024d69012485ed05d7eaa9617a96b81\"),\n",
    "        \"mono_no_pt_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip\",\n",
    "             \"9c2f071e35027c895a4728358ffc913a\"),\n",
    "        \"stereo_no_pt_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip\",\n",
    "             \"41ec2de112905f85541ac33a854742d1\"),\n",
    "        \"mono+stereo_no_pt_640x192\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip\",\n",
    "             \"46c3b824f541d143a45c37df65fbab0a\"),\n",
    "        \"mono_1024x320\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip\",\n",
    "             \"0ab0766efdfeea89a0d9ea8ba90e1e63\"),\n",
    "        \"stereo_1024x320\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip\",\n",
    "             \"afc2f2126d70cf3fdf26b550898b501a\"),\n",
    "        \"mono+stereo_1024x320\":\n",
    "            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip\",\n",
    "             \"cdc5fc9b23513c07d5b19235d9ef08f7\"),\n",
    "        }\n",
    "\n",
    "    if not os.path.exists(\"./monodept2/models\"):\n",
    "        os.makedirs(\"models\")\n",
    "\n",
    "    model_path = os.path.join(\"./monodept2/models\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m encoder \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mResnetEncoder(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m depth_decoder \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mDepthDecoder(num_ch_enc\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mnum_ch_enc, scales\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m loaded_dict_enc \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     13\u001b[0m feed_height \u001b[38;5;241m=\u001b[39m loaded_dict_enc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m feed_width \u001b[38;5;241m=\u001b[39m loaded_dict_enc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1040\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1272\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1270\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1271\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1272\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mactive_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1205\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     obj \u001b[38;5;241m=\u001b[39m cast(Storage, torch\u001b[38;5;241m.\u001b[39mUntypedStorage(nbytes))\n\u001b[1;32m   1204\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1205\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1209\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1210\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1211\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1316\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:268\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcuda(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from monodepth2 import networks\n",
    "\n",
    "# Cargar el modelo\n",
    "# download_model_if_doesnt_exist(model_name)\n",
    "model_path = os.path.join('./monodepth2/models',model_name)\n",
    "encoder_path = os.path.join(model_path,'encoder.pth')\n",
    "depth_decoder_path = os.path.join(model_path, 'depth.pth')\n",
    "\n",
    "encoder = networks.ResnetEncoder(18, False)\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location=device)    \n",
    "feed_height = loaded_dict_enc['height']\n",
    "feed_width = loaded_dict_enc['width']\n",
    "print(\"Altura:\", feed_height, \"Anchura:\", feed_width) #extraccion de las dimensiones de las imagenes con las que fue entrenado el modelo\n",
    "filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "depth_decoder.load_state_dict(torch.load(depth_decoder_path, map_location=device))\n",
    "depth_decoder.to(device)\n",
    "depth_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Predicting on 41 test images\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "if os.path.isdir(input_folder):\n",
    "    # Searching folder for images\n",
    "    paths = glob.glob(os.path.join(input_folder, '*.{}'.format('jpg')))\n",
    "    output_directory = input_folder\n",
    "else:\n",
    "    raise Exception(\"Can not find args.image_path: {}\".format(input_folder))\n",
    "\n",
    "print(\"-> Predicting on {:d} test images\".format(len(paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonodepth2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disp_to_depth\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonodepth2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEREO_SCALE_FACTOR\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# PREDICTING ON EACH IMAGE IN TURN\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Escritorio/TFG/monodepth2/evaluate_depth.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disp_to_depth\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m readlines\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MonodepthOptions\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layers'"
     ]
    }
   ],
   "source": [
    "from monodepth2.layers import disp_to_depth\n",
    "from monodepth2.evaluate_depth import STEREO_SCALE_FACTOR\n",
    "# PREDICTING ON EACH IMAGE IN TURN\n",
    "with torch.no_grad():\n",
    "    for idx, image_path in enumerate(paths):\n",
    "\n",
    "        if image_path.endswith(\"_disp.jpg\"):\n",
    "            # don't try to predict disparity for a disparity image!\n",
    "            continue\n",
    "\n",
    "        # Load image and preprocess\n",
    "        input_image = Image.open(image_path).convert('RGB')\n",
    "        original_width, original_height = input_image.size\n",
    "        input_image = input_image.resize((feed_width, feed_height), Image.LANCZOS)\n",
    "        input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "        # PREDICTION\n",
    "        input_image = input_image.to(device)\n",
    "        features = encoder(input_image)\n",
    "        outputs = depth_decoder(features)\n",
    "\n",
    "        disp = outputs[(\"disp\", 0)]\n",
    "        disp_resized = torch.nn.functional.interpolate(\n",
    "            disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Saving numpy file\n",
    "        output_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        scaled_disp, depth = disp_to_depth(disp, 0.1, 100)\n",
    "\n",
    "        name_dest_npy = os.path.join(output_folder, \"{}_depth.npy\".format(output_name))\n",
    "        metric_depth = STEREO_SCALE_FACTOR * depth.device().numpy()\n",
    "        np.save(name_dest_npy, metric_depth)\n",
    "\n",
    "        # Saving colormapped depth image\n",
    "        disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "        vmax = np.percentile(disp_resized_np, 95)\n",
    "        normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n",
    "        mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')\n",
    "        colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n",
    "        im = Image.fromarray(colormapped_im)\n",
    "\n",
    "        name_dest_im = os.path.join(output_directory, \"{}_disp.jpeg\".format(output_name))\n",
    "        im.save(name_dest_im)\n",
    "\n",
    "        print(\"   Processed {:d} of {:d} images - saved predictions to:\".format(\n",
    "            idx + 1, len(paths)))\n",
    "        print(\"   - {}\".format(name_dest_im))\n",
    "        print(\"   - {}\".format(name_dest_npy))\n",
    "\n",
    "print('-> Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import networks\n",
    "\n",
    "# Configuración de la carpeta de imágenes y resultados\n",
    "input_folder = './monocular_photos'\n",
    "output_folder = './depth_maps'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Cargar el modelo\n",
    "model_path = 'models/mono+stereo_640x192'\n",
    "encoder_path = os.path.join(model_path, 'encoder.pth')\n",
    "depth_decoder_path = os.path.join(model_path, 'depth.pth')\n",
    "\n",
    "encoder = networks.ResnetEncoder(18, False)\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "depth_decoder.load_state_dict(torch.load(depth_decoder_path, map_location='cpu'))\n",
    "\n",
    "encoder.eval()\n",
    "depth_decoder.eval()\n",
    "\n",
    "# Procesar imágenes\n",
    "with torch.no_grad():\n",
    "    for image_file in os.listdir(input_folder):\n",
    "        if image_file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            input_image = Image.open(os.path.join(input_folder, image_file)).convert('RGB')\n",
    "            original_width, original_height = input_image.size\n",
    "            input_image = input_image.resize((640, 192), Image.LANCZOS)\n",
    "            input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "            # Predicción de profundidad\n",
    "            features = encoder(input_image)\n",
    "            outputs = depth_decoder(features)\n",
    "\n",
    "            disp = outputs[(\"disp\", 0)]\n",
    "            disp_resized = torch.nn.functional.interpolate(\n",
    "                disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "            disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "\n",
    "            # Guardar el resultado\n",
    "            depth_filename = os.path.join(output_folder, os.path.splitext(image_file)[0] + '_depth.png')\n",
    "            Image.fromarray((disp_resized_np * 255).astype(np.uint8)).save(depth_filename)\n",
    "\n",
    "print(\"Procesamiento completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
